


# Technical decisions

### Compressing vs piping

To uncompress a file to stdout and pipe the output to a new file takes about 3m 20s and to remove the file (with `rm`) takes 4 seconds.

In contrast, to uncompress a file, and then recompress it takes 17m 23s (these are reported with `time`). Therefore the decision was taken to uncompress the file and redirect the output and then remove the temporary file!

### Uncompressing and running through awk

One of the issues we encountered was parallelizing the run through `awk`. 

This block of code in [uniref.snakefile](uniref.snakefile) has a couple of modifications to enable the multithreading:

```
       """
        RECORDS=$(cat {input.r});
        RPF=$((RECORDS/{params.n}));
        RPF=$((RPF+10));
        gunzip -c {input.u} | \
        awk -v rpf=$RPF -v fname={output.fn} 'BEGIN {{n=0;c=0}} \
                /^>/ {{ \
                    if(n%rpf==0){{ \
                        c++; \
                        file=sprintf("{params.d}/uniref50.split.%d.fa",c); \
                    }}; \
                    n++; \
                }} \
                {{ if (file==fname) {{ print >> file; }}; fflush(); }};'
        """
```

- Two variables are set for awk: `-v rpf=$RPF` sets the variable `rpf` to the shell variable `$RPF` (records per file) and `-v fname={output.fn}` sets the awk variable `fname` to the specific filename generated by snakemake.
   - The first of those is used to determine how many records to write per file (i.e. when to increment the file name)
   - The second of those is used to determine whether the thread we are in should write to a file. Each thread should only write to a single file, otherwise we stomp all over ourselves.
- The `fflush()` command is used to flush the [input and output buffers](https://www.gnu.org/software/gawk/manual/html_node/I_002fO-Functions.html). Without this flushing, the buffer causes issues with too many lines being written


